{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a method to compare the similarity of the Json output between Llama 3, Llama 3 - one shot, GPT 3.5, and my model while dealing with API endpoints IE (information extraction) task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8d02f1688b4d749db79b3a4b6e4f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, annotated-types, pydantic-core, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed annotated-types-0.7.0 pydantic-2.10.5 pydantic-core-2.27.2 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain)\n",
      "  Downloading langchain_core-0.3.30-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.29->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
      "Downloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.30-py3-none-any.whl (411 kB)\n",
      "Downloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.2.11-py3-none-any.whl (326 kB)\n",
      "Downloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading orjson-3.10.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Installing collected packages: tenacity, propcache, orjson, multidict, jsonpatch, greenlet, frozenlist, aiohappyeyeballs, yarl, SQLAlchemy, requests-toolbelt, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 greenlet-3.1.1 jsonpatch-1.33 langchain-0.3.14 langchain-core-0.3.30 langchain-text-splitters-0.3.5 langsmith-0.2.11 multidict-6.1.0 orjson-3.10.14 propcache-0.2.1 requests-toolbelt-1.0.0 tenacity-9.0.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.48.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.4.1+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (44.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (1.26.3)\n",
      "Collecting torch==2.5.1 (from xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->xformers) (9.1.0.70)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->xformers)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m163.6/188.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m163.6/188.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.30)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain-openai)\n",
      "  Downloading openai-1.59.7-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (0.2.11)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.58.1->langchain-openai)\n",
      "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-openai) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
      "Downloading langchain_openai-0.3.0-py3-none-any.whl (54 kB)\n",
      "Downloading openai-1.59.7-py3-none-any.whl (454 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Installing collected packages: jiter, tiktoken, openai, langchain-openai\n",
      "Successfully installed jiter-0.8.2 langchain-openai-0.3.0 openai-1.59.7 tiktoken-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepdiff\n",
      "  Downloading deepdiff-8.1.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting orderly-set<6,>=5.2.3 (from deepdiff)\n",
      "  Downloading orderly_set-5.2.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Downloading deepdiff-8.1.1-py3-none-any.whl (84 kB)\n",
      "Downloading orderly_set-5.2.3-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: orderly-set, deepdiff\n",
      "Successfully installed deepdiff-8.1.1 orderly-set-5.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic\n",
    "!pip install langchain\n",
    "!pip install scikit-learn\n",
    "!pip install sentence_transformers\n",
    "# !pip install xformers\n",
    "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install langchain-openai\n",
    "!pip install deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906f6783ec7b4c89a20ad5dccda4bbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59420f9646ad427e83ffc48a74ad7ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-d796dffda9188ea4.parquet:   0%|          | 0.00/785k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb978878c1b49399cda796c7dc8bb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a43ee893d543769dfa49e7912d1d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the same test/eval data while in training\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('billyfin/APIdoc2json')\n",
    "# delete the last line for future one-shot test\n",
    "one_shot_example = dataset['train'][166]\n",
    "dataset = dataset.filter(lambda example, idx: idx != 166, with_indices=True)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"MyIP.com JSON API Documentation\",\n",
      "    \"endpoints\": [\n",
      "        {\n",
      "            \"name\": \"Get IP Information\",\n",
      "            \"description\": \"Retrieves information about the IP address making the request.\",\n",
      "            \"method\": \"GET\",\n",
      "            \"url\": \"https://api.myip.com\",\n",
      "            \"headers\": [],\n",
      "            \"required_parameters\": [],\n",
      "            \"optional_parameters\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "JSON API | MyIP.com JSON API Contact JSON API You can make automated requests to the site using the API . Access URL: https://api.myip.com Response example: {\"ip\":\"66.249.75.9\",\"country\":\"United States\",\"cc\":\"US\"} Response elements: ip: IP address country: IP country location in English language cc: Two-letter country code in ISO 3166-1 alpha-2 format If there is no location data for an IP address cc will return \"XX\" and country \"Unknown\". Is this a free service? Yes. What are the API usage limits? There is no request limit, the only restriction is the server capacity which I will try to keep running smoothly. Can I use it for my commercial application? Yes, please give credit to myip.com if you can. How can I exclude some parameter(s) from the response? I will add that feature shortly, please check back for updates. I want you to add some feature, how can I contact you? If you have a suggestion please contact me here. MyIP.com 2024. × Contact For inquiries or feedback please get in touch at: OK\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset['json_form'][0])\n",
    "print(test_dataset['text_content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup, BitsAndBytesConfig\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftModel, PeftConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "torch.manual_seed(42)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc70d8cc6a940cd93c98f066cf2c9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"API text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    \n",
    "    result = outputs[0][\"generated_text\"]\n",
    "    with open(\"./model_outputs/llama3/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 - one shot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + one_shot_example['text_content'] + \"\\n\\nJson: \"},\n",
    "        {\"role\": \"assistant\", \"content\": one_shot_example['json_form']},\n",
    "        {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    \n",
    "    result = outputs[0][\"generated_text\"]\n",
    "    with open(\"./model_outputs/llama3_one_shot/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT3.5 - one shot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = str(input('Please type in your api key: '))\n",
    "\n",
    "count = 1\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        # model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + one_shot_example['text_content'] + \"\\n\\nJson: \"},\n",
    "            {\"role\": \"assistant\", \"content\": one_shot_example['json_form']},\n",
    "            {\"role\": \"user\", \"content\": \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" + test_sample + \"\\n\\nJson: \"},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = str(completion.choices[0].message.content)\n",
    "    with open(\"./model_outputs/gpt3.5_one_shot/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT3.5 + pydantic schema - one shot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import Optional, Any, Union, List\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    name: str = Field(description=\"Name of the parameter\")\n",
    "    type: str = Field(description=\"Type of the parameter\")\n",
    "    description: str = Field(description=\"Description of the parameter\")\n",
    "    default: Optional[Any] = Field(\n",
    "        description=\"Default value of the parameter\")\n",
    "    example: Optional[Any] = Field(\n",
    "        description=\"Example value of the parameter\")\n",
    "\n",
    "\n",
    "class Endpoint(BaseModel):\n",
    "    name: str = Field(description=\"Name of the endpoint\")\n",
    "    description: Optional[str] = Field(\n",
    "        description=\"Description of the endpoint\")\n",
    "    method: str = Field(description=\"Method of the endpoint\")\n",
    "    url: Union[str, List[str]] = Field(description=\"URL of the endpoint\")\n",
    "    headers: Optional[list] = Field(\n",
    "        default=[], description=\"Headers of the endpoint\")\n",
    "    required_parameters: list[Parameters]\n",
    "    optional_parameters: Optional[list[Parameters]]\n",
    "\n",
    "\n",
    "class Api_json(BaseModel):\n",
    "    title: Optional[str] = Field(description=\"Title of the API\")\n",
    "    endpoints: list[Endpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "OPENAI_API_KEY = str(input('Please type in your api key: '))\n",
    "count = 1\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0,\n",
    "                 openai_api_key=OPENAI_API_KEY,\n",
    "                 max_tokens=None,\n",
    "                )\n",
    "\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    prompt = MessagesPlaceholder(\"history\", optional=True)\n",
    "    prompt.format_messages()\n",
    "    prompt.format_messages(\n",
    "        history=[\n",
    "            (\"system\", \"You are an AI assistant.\"),\n",
    "            (\"human\", \"Hello!\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\",\n",
    "         \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: {api_doc} \\n\\nJson: \"),\n",
    "    ])\n",
    "\n",
    "    chain = prompt_template | llm.with_structured_output(\n",
    "        Api_json, method=\"function_calling\")\n",
    "    result = chain.invoke(\n",
    "        {\"history\":\n",
    "            [\n",
    "                (\"human\", \"You will be given an API documentation. Extract the endpoints and output in JSON format.\\n\\nAPI text content: \" +\n",
    "                 one_shot_example['text_content'] + \"\\n\\nJson: \"),\n",
    "                (\"ai\", one_shot_example['json_form']),\n",
    "            ],\n",
    "         \"api_doc\": test_sample\n",
    "        }\n",
    "    )\n",
    "    json = result.json(indent=4)\n",
    "    with open(\"./model_outputs/gpt3.5+pydantic_schema_one_shot/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(json)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APILlama outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(\"Current CUDA Device:\", device_name)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"billyfin/APILlama\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                            )\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10240\n",
    "\n",
    "def format(example):\n",
    "    input_messages = [\n",
    "        {\"role\":\"user\", \"content\": one_shot_example['text_content']},\n",
    "        {\"role\":\"assistant\", \"content\": one_shot_example['json_form']},\n",
    "        {\"role\":\"user\", \"content\": example},\n",
    "    ]\n",
    "    example = tokenizer.apply_chat_template(input_messages, tokenize=False) + \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    return example\n",
    "    \n",
    "def preprocess_for_inference(examples):\n",
    "    inputs = f\"{examples}\"\n",
    "    \n",
    "    model_inputs = tokenizer(inputs)\n",
    "    model_inputs['input_ids'] += [tokenizer.pad_token_id]\n",
    "    model_inputs[\"attention_mask\"] = [1] * len(model_inputs[\"input_ids\"])\n",
    "    \n",
    "    sample_input_ids = model_inputs[\"input_ids\"]\n",
    "    model_inputs[\"input_ids\"] = [tokenizer.pad_token_id] * (\n",
    "        max_length - len(sample_input_ids)\n",
    "    ) + sample_input_ids\n",
    "    model_inputs[\"attention_mask\"] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "        \"attention_mask\"\n",
    "    ]\n",
    "    model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"][:max_length])\n",
    "    model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"][:max_length])\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for test_sample in test_dataset['text_content']:\n",
    "    test_sample = format(test_sample)\n",
    "    test_input = preprocess_for_inference(test_sample)\n",
    "    inputs = {k: v.unsqueeze(0).to(device) for k, v in test_input.items()}\n",
    "    prompt = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0, prompt:], skip_special_tokens=True)\n",
    "    with open(\"./model_outputs/apillama/\" + str(count) + \".txt\", 'w') as file:\n",
    "        file.write(result)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Filter and extract json only\n",
    "def clean_json(str):\n",
    "    json_str = str.strip()\n",
    "    start_index = json_str.find('{')\n",
    "    json_type = 'object' if start_index != -1 else 'array'\n",
    "    end_index = json_str.rfind('}') if json_type == 'object' else json_str.rfind(']')\n",
    "    if start_index == -1:\n",
    "        start_index = json_str.find('[')\n",
    "        if start_index == -1:\n",
    "            raise ValueError(\"No JSON object or array found in the text\")\n",
    "    if end_index == -1:\n",
    "        raise ValueError(\"Incomplete JSON structure, no closing bracket found\")\n",
    "    \n",
    "    return json_str[start_index:end_index+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the accuracy of the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, ValidationError, Field\n",
    "from typing import Optional, Any, Union, List\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    name: str = Field(description=\"Name of the parameter\")\n",
    "    type: str = Field(description=\"Type of the parameter\")\n",
    "    description: str = Field(description=\"Description of the parameter\")\n",
    "    default: Optional[Any] = Field(\n",
    "        description=\"Default value of the parameter\")\n",
    "    example: Optional[Any] = Field(\n",
    "        description=\"Example value of the parameter\")\n",
    "\n",
    "class Endpoint(BaseModel):\n",
    "    name: str = Field(description=\"Name of the endpoint\")\n",
    "    description: Optional[str] = Field(\n",
    "        description=\"Description of the endpoint\")\n",
    "    method: str = Field(description=\"Method of the endpoint\")\n",
    "    url: Union[str] = Field(description=\"URL of the endpoint\")\n",
    "    headers: Optional[list] = Field(\n",
    "        default=[], description=\"Headers of the endpoint\")\n",
    "    required_parameters: list[Parameters]\n",
    "    optional_parameters: Optional[list[Parameters]]\n",
    "\n",
    "class Api_json(BaseModel):\n",
    "    title: Optional[str] = Field(description=\"Title of the API\")\n",
    "    endpoints: list[Endpoint]\n",
    "\n",
    "def check_format(generated):\n",
    "    try:\n",
    "        generated = Api_json(**generated)\n",
    "    except ValidationError as e:\n",
    "        return False\n",
    "    return True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# Checks if the structure and format of the predicted JSON is corresponding to the one-shot example\n",
    "json_truths = test_dataset['json_form']\n",
    "format_truth = []\n",
    "format_prediction = []\n",
    "\n",
    "gpt_format_result = []\n",
    "my_model_format_result = []\n",
    "result = {}\n",
    "directory = './model_outputs/'\n",
    "prediction_folders = [f for f in os.listdir(directory)]\n",
    "\n",
    "for folder in prediction_folders:\n",
    "\n",
    "    print(\"Evaluating folder: {}\".format(folder))\n",
    "    if folder.startswith('.'):\n",
    "        continue\n",
    "    for i in range(len(json_truths)):\n",
    "        with open(directory + folder + \"/\" + str(i + 1) + \".txt\", 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        try:\n",
    "            json_content = json.loads(clean_json(content))\n",
    "            truth = json.loads(json_truths[i])\n",
    "        except Exception as e:\n",
    "            format_prediction.append(False)\n",
    "            format_truth.append(check_format(truth))\n",
    "            continue\n",
    "\n",
    "        format_prediction.append(check_format(json_content))\n",
    "        format_truth.append(check_format(truth))\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    if False in format_truth:\n",
    "        print(\"Some of the truth JSONs are not in the correct format\")\n",
    "    accuracy = accuracy_score(format_truth, format_prediction)\n",
    "    print(\"Accuracy of {} is: {}\".format(folder, accuracy), end='\\n\\n')\n",
    "    # Save the results\n",
    "    if folder == \"apillama\":\n",
    "        my_model_format_result = format_prediction\n",
    "    elif folder == \"gpt3.5+pydantic_schema_one_shot\":\n",
    "        gpt_format_result = format_prediction\n",
    "    result[folder] = format_prediction\n",
    "    # Reset the lists\n",
    "    format_truth = []\n",
    "    format_prediction = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the precision, recall, and the F1 of matched urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_precision_recall_f1(truth, generated):\n",
    "    # Combine both lists to create a unique list of URLs\n",
    "    all_urls = list(set(truth) | set(generated))\n",
    "    # Create a list of 1s and 0s for the ground truth and the generated URLs\n",
    "    y_true = [1 if url in truth else 0 for url in all_urls]\n",
    "    y_pred = [1 if url in generated else 0 for url in all_urls]\n",
    "    # Calculate the precision, recall and f1 score\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    success_total = sum(y_pred)\n",
    "    return precision, recall, f1, success_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdiff import DeepSearch\n",
    "\n",
    "def search(obj, key):\n",
    "    list = []\n",
    "    ds = DeepSearch(obj, key, verbose_level=2)\n",
    "    for key in ds['matched_paths']:\n",
    "        if key.endswith(\"['url']\"):\n",
    "            list.append(ds['matched_paths'][key])\n",
    "    return list\n",
    "\n",
    "# intersection = []\n",
    "# for i in range(len(json_truths)):\n",
    "#     if my_model_format_result[i] == gpt_format_result[i] == True:\n",
    "#         intersection.append(i)\n",
    "\n",
    "item = \"url\"\n",
    "for folder in [\"apillama/\", \"gpt3.5_one_shot/\",\"gpt3.5+pydantic_schema_one_shot/\", \"llama3/\", \"llama3_one_shot/\"]:\n",
    "    # reset the lists\n",
    "    truth_urls = []\n",
    "    prediction_urls = []\n",
    "    matching_list = result[folder[:-1]]\n",
    "    intersection = []\n",
    "    for i in range(len(matching_list)):\n",
    "        if matching_list[i] == True:\n",
    "            intersection.append(i)\n",
    "    print(\"Evaluating folder: {}\".format(folder))\n",
    "    for i in intersection:\n",
    "        truth = json.loads(json_truths[i])\n",
    "        truth_urls.extend(search(truth, item))\n",
    "\n",
    "        with open(directory + folder + str(i + 1) + \".txt\", 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        prediction = json.loads(clean_json(content))\n",
    "        try:\n",
    "            prediction_urls.extend(search(prediction, item))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    precision, recall, f1, success_total = compute_precision_recall_f1(truth_urls, prediction_urls)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Success_total: \", success_total, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import xformers\n",
    "\n",
    "model = SentenceTransformer(\"infgrad/stella_en_400M_v5\", trust_remote_code=True).cuda()\n",
    "\n",
    "def semantic_similarity(generated, truth):\n",
    "    global model\n",
    "    docs = [\n",
    "        generated,\n",
    "        truth\n",
    "    ]\n",
    "    doc_embeddings = model.encode(docs)\n",
    "    similarities = model.similarity(doc_embeddings, doc_embeddings)\n",
    "    return similarities[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [\"apillama/\", \"gpt3.5_one_shot/\",\"gpt3.5+pydantic_schema_one_shot/\", \"llama3/\", \"llama3_one_shot/\"]:\n",
    "    # reset the variables\n",
    "    total_name_cosine = 0\n",
    "    total_description_cosine = 0\n",
    "    total_method_list = []\n",
    "    total_req_precision = 0\n",
    "    total_req_recall = 0\n",
    "    total_req_num = 0\n",
    "    total_opt_precision = 0\n",
    "    total_opt_recall = 0\n",
    "    total_opt_num = 0\n",
    "    total_req_description_cosine = 0\n",
    "    total_opt_description_cosine = 0\n",
    "    total_req_type_accuracy = 0\n",
    "    total_opt_type_accuracy = 0\n",
    "    num_endpoints = 0\n",
    "    endpoint_req_description_size = 0\n",
    "    endpoint_opt_description_size = 0\n",
    "    endpoint_req_type_size = 0\n",
    "    endpoint_opt_type_size = 0\n",
    "    print(\"Evaluating folder: {}\".format(folder))\n",
    "    matching_list = result[folder[:-1]]\n",
    "    intersection = []\n",
    "    for i in range(len(matching_list)):\n",
    "        if matching_list[i] == True:\n",
    "            intersection.append(i)\n",
    "    # print(intersection)\n",
    "    for i in intersection:\n",
    "\n",
    "        truth = json.loads(json_truths[i])\n",
    "        truth_endpoints = truth['endpoints']\n",
    "        with open(directory + folder + str(i + 1) + \".txt\", 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        prediction = json.loads(clean_json(content))\n",
    "        prediction_endpoints = prediction['endpoints']\n",
    "\n",
    "        for truth_endpoint in truth_endpoints:\n",
    "            endpoint_req_descriptions_cosine = 0\n",
    "            endpoint_opt_descriptions_cosine = 0\n",
    "            endpoint_req_type_list = []\n",
    "            endpoint_opt_type_list = []\n",
    "\n",
    "            truth_name = truth_endpoint['name']\n",
    "            truth_description = truth_endpoint['description']\n",
    "            truth_req_params = truth_endpoint['required_parameters']\n",
    "            truth_opt_params = truth_endpoint['optional_parameters']\n",
    "            matching_endpoint = next((endpoint for endpoint in prediction_endpoints if endpoint['url'] == truth_endpoint['url']), None)\n",
    "            if matching_endpoint:\n",
    "                num_endpoints += 1\n",
    "                total_name_cosine += semantic_similarity(matching_endpoint['name'], truth_name)\n",
    "                total_description_cosine += semantic_similarity(matching_endpoint['description'], truth_description)\n",
    "                total_method_list.append(matching_endpoint['method'] == truth_endpoint['method'])\n",
    "                # calculate the precision and recall score for the parameters\n",
    "                truth_req_param_names = [param['name'] for param in truth_req_params]\n",
    "                generated_req_param_names = [param['name'] for param in matching_endpoint['required_parameters']]\n",
    "                truth_opt_param_names = [param['name'] for param in truth_opt_params]\n",
    "                try:\n",
    "                    generated_opt_param_names = [param['name'] for param in matching_endpoint['optional_parameters']] if matching_endpoint['optional_parameters'] else []\n",
    "                except:\n",
    "                    generated_opt_param_names = []\n",
    "                req_precision, req_recall, req_f1, req_num = compute_precision_recall_f1(truth_req_param_names, generated_req_param_names) if not (truth_req_params == generated_req_param_names == []) else (1, 1, 1, 0)                \n",
    "                opt_precision, opt_recall, opt_f1, opt_num = compute_precision_recall_f1(truth_opt_param_names, generated_opt_param_names) if not (truth_opt_params == generated_opt_param_names == []) else (1, 1, 1, 0)\n",
    "                total_req_precision += req_precision\n",
    "                total_req_recall += req_recall\n",
    "                total_req_num += req_num\n",
    "                total_opt_precision += opt_precision\n",
    "                total_opt_recall += opt_recall\n",
    "                total_opt_num = opt_num\n",
    "                # calculate the cosine similarity of the parameter descriptions and types\n",
    "                req_count = 0\n",
    "                opt_count = 0\n",
    "                req_type_count = 0\n",
    "                opt_type_count = 0\n",
    "                for name in truth_req_param_names:\n",
    "                    matched_description = next((param['description'] for param in matching_endpoint['required_parameters'] if param['name'] == name), None)\n",
    "                    matched_type = next((param['type'] for param in matching_endpoint['required_parameters'] if param['name'] == name), None)\n",
    "                    if matched_description:\n",
    "                        req_count += 1\n",
    "                        endpoint_req_descriptions_cosine += semantic_similarity(matched_description, next(param['description'] for param in truth_req_params if param['name'] == name))\n",
    "                    if matched_type:\n",
    "                        req_type_count += 1\n",
    "                        endpoint_req_type_list.append(matched_type == next(param['type'] for param in truth_req_params if param['name'] == name))\n",
    "                if req_type_count != 0:\n",
    "                    endpoint_req_type_size += 1\n",
    "                if req_count != 0:\n",
    "                    # increment the size of the description for the average calculation\n",
    "                    endpoint_req_description_size += 1\n",
    "                for name in truth_opt_param_names:\n",
    "                    # check if the optional parameter is present in the generated parameters\n",
    "                    if matching_endpoint['optional_parameters']:\n",
    "                        matched_description = next((param['description'] for param in matching_endpoint['optional_parameters'] if param['name'] == name), None)\n",
    "                        if matched_description:\n",
    "                            opt_count += 1\n",
    "                            endpoint_opt_descriptions_cosine += semantic_similarity(matched_description, next(param['description'] for param in truth_opt_params if param['name'] == name))\n",
    "                        matched_type = next((param['type'] for param in matching_endpoint['optional_parameters'] if param['name'] == name), None)\n",
    "                        if matched_type:\n",
    "                            opt_type_count += 1\n",
    "                            endpoint_opt_type_list.append(matched_type == next(param['type'] for param in truth_opt_params if param['name'] == name))\n",
    "                if opt_type_count != 0:\n",
    "                    endpoint_opt_type_size += 1\n",
    "                if opt_count != 0:\n",
    "                    # increment the size of the description for the average calculation\n",
    "                    endpoint_opt_description_size += 1\n",
    "                \n",
    "            # calculate the average cosine similarity for the descriptions\n",
    "            total_req_description_cosine += (endpoint_req_descriptions_cosine / req_count if req_count != 0 else 0)\n",
    "            total_opt_description_cosine += (endpoint_opt_descriptions_cosine / opt_count if opt_count != 0 else 0)\n",
    "            # calculate the average type accuracy\n",
    "            total_req_type_accuracy += sum(endpoint_req_type_list) / len(endpoint_req_type_list) if len(endpoint_req_type_list) != 0 else 0\n",
    "            total_opt_type_accuracy += sum(endpoint_opt_type_list) / len(endpoint_opt_type_list) if len(endpoint_opt_type_list) != 0 else 0\n",
    "    epsilon = 1e-6\n",
    "    num_endpoints += epsilon\n",
    "    endpoint_req_description_size += epsilon\n",
    "    endpoint_opt_description_size += epsilon\n",
    "    endpoint_req_type_size += epsilon\n",
    "    endpoint_opt_type_size += epsilon\n",
    "    \n",
    "    total_name_cosine /= num_endpoints\n",
    "    total_description_cosine /= num_endpoints\n",
    "    total_method_accuracy = sum(total_method_list) / num_endpoints\n",
    "    total_req_precision /= num_endpoints\n",
    "    total_req_recall /= num_endpoints\n",
    "    total_opt_precision /= num_endpoints\n",
    "    total_opt_recall /= num_endpoints\n",
    "    total_req_description_cosine /= endpoint_req_description_size\n",
    "    total_opt_description_cosine /= endpoint_opt_description_size\n",
    "    total_req_type_accuracy /= endpoint_req_type_size\n",
    "    total_opt_type_accuracy /= endpoint_opt_type_size\n",
    "\n",
    "    print(\"Endpoint Name Cosine Similarity: %.2f\" % total_name_cosine)\n",
    "    print(\"Description Cosine Similarity: %.2f\" % total_description_cosine)\n",
    "    print(\"Method Accuracy: %.2f\" % total_method_accuracy)\n",
    "    print(\"Required Parameters Precision: %.2f\" % total_req_precision)\n",
    "    print(\"Required Parameters Recall: %.2f\" % total_req_recall)\n",
    "    print(\"Total correct required parameters: %d\" % total_req_num)\n",
    "    print(\"Optional Parameters Precision: %.2f\" % total_opt_precision)\n",
    "    print(\"Optional Parameters Recall: %.2f\" % total_opt_recall)\n",
    "    print(\"Total correct optional parameters: %d\" % total_opt_num)\n",
    "    print(\"Required Parameters Description Cosine Similarity: %.2f\" % total_req_description_cosine)\n",
    "    print(\"Optional Parameters Description Cosine Similarity: %.2f\" % total_opt_description_cosine)\n",
    "    print(\"Required Parameters Type Accuracy: %.2f\" % total_req_type_accuracy)\n",
    "    print(\"Optional Parameters Type Accuracy: %.2f\" % total_opt_type_accuracy, end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
