{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Tool Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'[sequence]': {('glycam',\n",
       "               'DManpa1-2DManpa1-3DManpa1-6[DManpa1-3]DManpb1-4DGlcpNAcb1-4DGlcpNAcb1-OH'),\n",
       "              ('glycam-new', 'DManpa1-6DManpa1-OH'),\n",
       "              ('glycam2', 'DManpa1-6DManpa1-OH')},\n",
       "             '[buildOptions][conformers]': {('glycam-new', '1ogg'),\n",
       "              ('glycam2', 64)},\n",
       "             '[pUUID]': {('glycam-new',\n",
       "               '3c368bf2-ad73-43f3-a18d-d7d2dc11cf28'),\n",
       "              ('glycam2', '3c368bf2-ad73-43f3-a18d-d7d2dc11cf28')},\n",
       "             '[conformerID]': {('glycam-new', '1ogg'), ('glycam2', '1ogg')},\n",
       "             '[category-name]': {('example', 'food-and-drink')},\n",
       "             '[group-name]': {('example', 'animal-bird')}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = os.listdir(\"../extractor/apidocs\")\n",
    "parameter_dict = defaultdict(set)\n",
    "def add_to_dict(d: defaultdict, prev_path: str, object, api_doc: str):\n",
    "    '''encode a json tree into a dictionary with root to leaf path. sets have max length of 10'''\n",
    "    if isinstance(object, dict):\n",
    "        for key, value in object.items():\n",
    "            add_to_dict(d, prev_path + \"[\" + key + \"]\", value, api_doc)\n",
    "    elif isinstance(object, list):\n",
    "        for item in object:\n",
    "            add_to_dict(d, prev_path, item, api_doc)\n",
    "    else:\n",
    "        if object:\n",
    "            if len(d[prev_path]) < 10:\n",
    "                d[prev_path].add((api_doc, object))\n",
    "def build_dict(d, object, api_doc):\n",
    "    add_to_dict(d, '', object, api_doc)\n",
    "for folder in folders:\n",
    "    path = \"../extractor/apidocs/\" + folder + \"/\" + folder + \".txt\"\n",
    "    try:\n",
    "        json_file = json.load(open(path))\n",
    "    except:\n",
    "        continue\n",
    "    for endpoint in json_file['endpoints']:\n",
    "        for param in endpoint['required_parameters']:\n",
    "            name = param['name']\n",
    "            example = None if not param['example'] else param['example']\n",
    "            add_to_dict(parameter_dict, \"[\"+name+\"]\", example, folder)\n",
    "            \n",
    "        if endpoint['optional_parameters']:\n",
    "            for param in endpoint['optional_parameters']:\n",
    "                name = param['name']\n",
    "                example = None if not param['example'] else param['example']\n",
    "                add_to_dict(parameter_dict, \"[\"+name+\"]\", example, folder)\n",
    "parameter_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The sequence to be evaluated.': 'sequence',\n",
       " 'The sequence for which to build the 3D structure.': 'sequence',\n",
       " 'Options for building the 3D structure, such as number of conformers.': 'buildOptions',\n",
       " 'The project UUID to check the status for.': 'pUUID',\n",
       " 'The conformer ID to check the status for.': 'conformerID',\n",
       " 'The project UUID for the structure to download.': 'pUUID',\n",
       " 'The name of the category to filter emojis by.': 'category-name',\n",
       " 'The name of the group to filter emojis by.': 'group-name',\n",
       " 'Options for building the structure, such as specific conformers.': 'buildOptions',\n",
       " 'The ID of the conformer to check the status for.': 'conformerID',\n",
       " 'The sequence of the sugar expressed in condensed glycam notation.': 'sequence'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = os.listdir(\"../extractor/apidocs\")\n",
    "description_to_param_dict = {} # map description to parameter\n",
    "param_to_description_dict = {} # map parameter to description\n",
    "for folder in folders:\n",
    "    path = \"../extractor/apidocs/\" + folder + \"/\" + folder + \".txt\"\n",
    "    try:\n",
    "        json_file = json.load(open(path))\n",
    "    except:\n",
    "        continue\n",
    "    for endpoint in json_file['endpoints']:\n",
    "        for param in endpoint['required_parameters']:\n",
    "            name = param['name']\n",
    "            description = param['description']\n",
    "            description_to_param_dict[description] = name\n",
    "            param_to_description_dict[name] = description\n",
    "        if endpoint['optional_parameters']:\n",
    "            for param in endpoint['optional_parameters']:\n",
    "                name = param['name']\n",
    "                description = param['description']\n",
    "                description_to_param_dict[description] = name\n",
    "                param_to_description_dict[name] = description\n",
    "\n",
    "description_to_param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get API response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested: poll_project_status_GET.py\n",
      "Tested: poll_specific_build_status_GET.py\n",
      "Tested: download_structure_GET.py\n",
      "Tested: evaluate_sequence_POST.py\n",
      "Tested: build_3d_structure_POST.py\n",
      "Tested: get_emojis_by_group_GET.py\n",
      "Tested: get_random_emoji_GET.py\n",
      "Tested: get_emojis_by_category_GET.py\n",
      "Tested: get_all_emojis_GET.py\n",
      "Tested: poll_project_status_GET.py\n",
      "Tested: poll_specific_build_status_GET.py\n",
      "Tested: download_structure_GET.py\n",
      "Tested: evaluate_sequence_POST.py\n",
      "Tested: build_3d_structure_POST.py\n",
      "Tested: build_structure_via_url_GET.py\n"
     ]
    }
   ],
   "source": [
    "# no need to run again\n",
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "count = 0\n",
    "evaluation_result={}\n",
    "folders = os.listdir(apidocs_dir)\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    failed_endpoints = []\n",
    "    api_result={}\n",
    "    files = [x for x in files if x.endswith(\".py\")]\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".py\"):\n",
    "            count += 1\n",
    "            file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python\", file_path], capture_output=True, text=True, check=True\n",
    "                )\n",
    "                if result.stdout:\n",
    "                    output = result.stdout\n",
    "                    output_json = json.loads(output)\n",
    "                    # save in file\n",
    "                    with open(file_path[:-3] + '_response.json', \"w\") as f:\n",
    "                        json.dump(output_json, f)\n",
    "                else:\n",
    "                    pass\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                pass\n",
    "            print(f\"Tested: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "count = 0\n",
    "evaluation_result={}\n",
    "folders = os.listdir(apidocs_dir)\n",
    "response_dict = defaultdict(set)\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    failed_endpoints = []\n",
    "    api_result={}\n",
    "    files = [x for x in files if x.endswith(\"_response.json\")]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "        response = json.load(open(file_path))\n",
    "        if response['status_code'] == 200:\n",
    "            response_json = response['json']\n",
    "            build_dict(response_dict, response_json, folder)\n",
    "response_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate embedding, build parameter knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhaonan/Desktop/research/APIAgent/.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\"flax-sentence-embeddings/st-codesearch-distilroberta-base\")\n",
    "param_keys = list(parameter_dict.keys())\n",
    "param_keys_emb = model.encode(param_keys, convert_to_tensor=True)\n",
    "response_keys = list(response_dict.keys())\n",
    "response_keys_emb = model.encode(response_keys, convert_to_tensor=True)\n",
    "description_keys = list(description_to_param_dict.keys())\n",
    "description_keys_emb = model.encode(description_keys, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'iupacextended'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m hits_param = util.semantic_search(query_emb, param_keys_emb)\n\u001b[32m      4\u001b[39m hits_response = util.semantic_search(query_emb, response_keys_emb)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m query_description = \u001b[43mparam_to_description_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m query_description_emb = model.encode(query_description, convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m hits_description = util.semantic_search(query_description_emb, description_keys_emb)\n",
      "\u001b[31mKeyError\u001b[39m: 'iupacextended'"
     ]
    }
   ],
   "source": [
    "query = 'iupacextended'\n",
    "query_emb = model.encode(query, convert_to_tensor=True) \n",
    "hits_param = util.semantic_search(query_emb, param_keys_emb)\n",
    "hits_response = util.semantic_search(query_emb, response_keys_emb)\n",
    "query_description = param_to_description_dict[query]\n",
    "query_description_emb = model.encode(query_description, convert_to_tensor=True)\n",
    "hits_description = util.semantic_search(query_description_emb, description_keys_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hits_param[0]\n",
    "for hit in h:\n",
    "    print(f\"Param: {param_keys[hit['corpus_id']]} Example: {parameter_dict[param_keys[hit['corpus_id']]]}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"==================================\")\n",
    "h = hits_response[0]\n",
    "for hit in h:\n",
    "    print(f\"Request: {response_keys[hit['corpus_id']]} Example: {response_dict[response_keys[hit['corpus_id']]]}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"==================================\")\n",
    "h = hits_description[0]\n",
    "for hit in h:\n",
    "    print(f\"Description: {description_keys[hit['corpus_id']]} Example: {parameter_dict['['+description_to_param_dict[description_keys[hit['corpus_id']]]+']']}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load generated tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def load_and_import(api_name, function_name):\n",
    "    folder_path = os.path.join(\"..\",\"extractor\", \"apidocs\", api_name)\n",
    "    sys.path.insert(0, folder_path)\n",
    "    module = __import__(function_name)\n",
    "    sys.path.pop(0)  # Clean up after import\n",
    "    return module\n",
    "\n",
    "module = load_and_import(\"glycanformatconverter\", \"convert_glycoct_to_wurcs_GET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = getattr(module, \"convert_glycoct_to_wurcs\")\n",
    "response = f(glycoct='''RES\\n1b:x-dglc-HEX-1:5\\n2s:n-acetyl\\n3b:b-dglc-HEX-1:5\\n4s:n-acetyl\\n5b:b-dman-HEX-1:5\\n6b:a-dman-HEX-1:5\\n7b:a-dman-HEX-1:5\\nLIN\\n1:1d(2+1)2n\\n2:1o(4+1)3d\\n3:3d(2+1)4n\\n4:3o(4+1)5d\\n5:5o(3+1)6d\\n6:5o(6+1)7d''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query KB and get parameter value candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(api_name, endpoint_name):\n",
    "    function_name = endpoint_name.replace(\"_GET\", \"\").replace(\"_POST\", \"\")\n",
    "    module = load_and_import(api_name, endpoint_name)\n",
    "    f = getattr(module, function_name)\n",
    "    return f\n",
    "\n",
    "import random\n",
    "def get_test_examples(query, api_doc_name, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5, query_description=None):\n",
    "    if not query_description:\n",
    "        try:\n",
    "            query_description = param_to_description_dict[query]\n",
    "        except:\n",
    "            query_description = None\n",
    "    query_emb = model.encode(query, convert_to_tensor=True) \n",
    "    hits_param = util.semantic_search(query_emb, param_keys_emb)\n",
    "    hits_response = util.semantic_search(query_emb, response_keys_emb)\n",
    "    if query_description:\n",
    "        query_description_emb = model.encode(query_description, convert_to_tensor=True)\n",
    "        hits_description = util.semantic_search(query_description_emb, description_keys_emb)\n",
    "    hit_list = hits_param[0]\n",
    "    all_examples = set()\n",
    "    count = 0\n",
    "    for hit in hit_list:\n",
    "        if hit['score'] < similarity_threshold or count > max_param_examples:\n",
    "            break\n",
    "        param_key = param_keys[hit['corpus_id']]\n",
    "        examples_set = parameter_dict[param_key]\n",
    "        filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "        # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "        examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "        sidx = param_key.rfind('[')+1\n",
    "        test_param = param_key[sidx:-1]\n",
    "        # print(test_param, examples)\n",
    "        pairs = [(test_param, example) for example in examples]\n",
    "        all_examples.update(pairs)\n",
    "        # print(all_examples)\n",
    "        count += len(examples)\n",
    "    hit_list = hits_response[0]\n",
    "    count = 0\n",
    "    for hit in hit_list:\n",
    "        if hit['score'] < similarity_threshold or count > max_response_examples:\n",
    "            break\n",
    "        response_key = response_keys[hit['corpus_id']]\n",
    "        examples_set = response_dict[response_key]\n",
    "        examples_set_copy = examples_set.copy()\n",
    "        filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "        # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "        examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "        sidx = response_key.rfind('[')+1\n",
    "        test_param = response_key[sidx:-1]\n",
    "        # print(test_param, examples)\n",
    "        pairs = [(test_param, example) for example in examples]\n",
    "        all_examples.update(pairs)\n",
    "        # print(all_examples)\n",
    "        count += len(examples)\n",
    "    if query_description:\n",
    "        hit_list = hits_description[0]\n",
    "        count = 0\n",
    "        for hit in hit_list:\n",
    "            if hit['score'] < similarity_threshold or count > max_param_examples:\n",
    "                break\n",
    "            description_key = description_keys[hit['corpus_id']]\n",
    "            param_key = description_to_param_dict[description_key]\n",
    "            examples_set = parameter_dict['['+param_key+']']\n",
    "            filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "            # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "            examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "            # print(param_key, examples)\n",
    "            pairs = [(param_key, example) for example in examples]\n",
    "            all_examples.update(pairs)\n",
    "            # print(all_examples)\n",
    "            count += len(examples)\n",
    "    return all_examples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_examples(\"iupacextended\", \"glycanformatconverter\", max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "import itertools\n",
    "def get_parameter_names(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return [param.name for param in signature.parameters.values()]\n",
    "def run_experiment(MAX_SAMPLES=100):\n",
    "    apidocs_dir = \"../extractor/apidocs\"\n",
    "    folders = os.listdir(apidocs_dir)\n",
    "    for folder in folders:\n",
    "        files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "        files = [x for x in files if x.endswith(\".py\")]\n",
    "        api_extracted_json = {}\n",
    "        with open(os.path.join(apidocs_dir, folder, folder + \".txt\")) as f:\n",
    "            api_extracted_json = json.load(f)\n",
    "        extracted_endpoints = None\n",
    "        try:\n",
    "            extracted_endpoints = api_extracted_json['endpoints']\n",
    "        except:\n",
    "            print(\"No extracted information for api: \", folder)\n",
    "            extracted_endpoints = None\n",
    "            continue\n",
    "        for idx, file_name in enumerate(files):\n",
    "            endpoint_name = file_name.replace(\".py\", \"\")\n",
    "            record_file_name = endpoint_name + \"_example_test.json\"\n",
    "            if os.path.exists(os.path.join(apidocs_dir, folder, record_file_name)):\n",
    "                endpoint_result = json.load(open(os.path.join(apidocs_dir, folder, record_file_name)))\n",
    "            else:\n",
    "                endpoint_result = {\"endpoint\": endpoint_name, \"tests\": [], \"extracted_parameters\": {}, \"validated_parameters\": {}}\n",
    "                print(f\"Testing {folder}.{endpoint_name}\")\n",
    "                if file_name == \"__init__.py\":\n",
    "                    continue\n",
    "                \n",
    "            if extracted_endpoints:\n",
    "                # find the ground truth parameters\n",
    "                pass\n",
    "                # endpoint_result['extracted_parameters'] = {}\n",
    "                # current_endpoint = None\n",
    "                # for endpoint in extracted_endpoints:\n",
    "                #     current_endpoint_name = endpoint['name'].replace(' ','_').lower() + '_' + endpoint['method']\n",
    "                #     if current_endpoint_name == endpoint_name:\n",
    "                #         current_endpoint = endpoint\n",
    "                #         print(f\"Found endpoint {endpoint_name} in extracted information\")\n",
    "                #         break\n",
    "                # if not current_endpoint:\n",
    "                #     print(f\"Endpoint {endpoint_name} not found in extracted information\")\n",
    "                #     continue\n",
    "                # for req_param in current_endpoint['required_parameters']:\n",
    "                #     endpoint_result['extracted_parameters'][req_param['name']] = {'description': req_param['description'], 'example': req_param['example']}\n",
    "                # if current_endpoint['optional_parameters']:\n",
    "                #     for opt_param in current_endpoint['optional_parameters']:\n",
    "                #         endpoint_result['extracted_parameters'][opt_param['name']] = {'description': opt_param['description'], 'example': opt_param['example']}\n",
    "            f = get_function(folder, endpoint_name)\n",
    "            param_names = get_parameter_names(f)\n",
    "            # simulate the situation that no example is provided\n",
    "            # for a single parameter function, we can try a few test cases.\n",
    "            if len(param_names) == 1:\n",
    "                endpoint_result['tests'] = []\n",
    "                curr_param = param_names[0]\n",
    "                examples = get_test_examples(curr_param, folder, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)\n",
    "                for test_param, example in examples:\n",
    "                    try:\n",
    "                        response = f(example)\n",
    "                    except:\n",
    "                        print(f'Error occured when calling {folder}.{endpoint_name} with {curr_param}={example}')\n",
    "                        continue\n",
    "                    response_json =\"\"\n",
    "                    try:\n",
    "                        response_json = response.json()\n",
    "                    except:\n",
    "                        pass\n",
    "                    example_result = {'gt_param': curr_param,'test_param': test_param, 'candidate': example, 'status_code': response.status_code, 'json': response_json, 'text': response.text}\n",
    "                    endpoint_result['tests'].append(example_result)\n",
    "                    time.sleep(0.1)\n",
    "                pass \n",
    "            else:\n",
    "                # use a rough estimation, record the ranking of the ground truth parameter(?)\n",
    "                endpoint_result['tests'] = []\n",
    "                example_list = []\n",
    "                for param in param_names:\n",
    "                    examples = get_test_examples(param, folder, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)\n",
    "                    example_list.append(examples)\n",
    "                candidate_combinations = list(itertools.product(*example_list))\n",
    "                samples = random.sample(candidate_combinations, min(MAX_SAMPLES, len(candidate_combinations)))\n",
    "                print(f\"Testing {folder}.{endpoint_name} with {len(samples)} samples\")\n",
    "                for sample in samples:\n",
    "                    try:\n",
    "                        input_dict = {param_names[i]: sample[i][1] for i in range(len(param_names))}\n",
    "                        response = f(**input_dict)\n",
    "                    except:\n",
    "                        print(f'Error occured when calling {folder}.{endpoint_name} with {input_dict}')\n",
    "                        continue\n",
    "                    response_json =\"\"\n",
    "                    try:\n",
    "                        response_json = response.json()\n",
    "                    except:\n",
    "                        pass\n",
    "                    example_result = {'gt_param': param_names,'test_param': [t[0] for t in sample], 'candidate': [t[1] for t in sample], 'status_code': response.status_code, 'json': response_json, 'text': response.text}\n",
    "                    endpoint_result['tests'].append(example_result)\n",
    "                    time.sleep(0.1)\n",
    "                pass\n",
    "            # save endpoint_result\n",
    "            save_path = os.path.join(apidocs_dir, folder, endpoint_name + \"_example_test.json\")\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(endpoint_result, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let gpt evaluate the tested examples\n",
    "1. check if status code is 200\n",
    "2. chunk the response\n",
    "3. sent response to gpt, let it check if the API returns an actual response rather than an error information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Decide if the following API response is an information or an error message.\n",
    "\n",
    "API Description:\n",
    "{description}\n",
    "API Response:\n",
    "{response}\n",
    "\"\"\"\n",
    ")\n",
    "class Classification(BaseModel):\n",
    "    response_type: str= Field(..., enum=['information', 'error'])\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(Classification)\n",
    "\n",
    "def gpt_evaluate(API_description:str, API_response: str):\n",
    "    '''let gpt to decide if the API response is a piece of information or an error message'''\n",
    "    prompt = tagging_prompt.invoke({\"description\": API_description, \"response\": API_response})\n",
    "    gpt_response = llm.invoke(prompt)\n",
    "    return gpt_response.response_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "folders = os.listdir(apidocs_dir)\n",
    "count_single_param_files = 0\n",
    "count_multi_param_files = 0\n",
    "count_validated_files = 0\n",
    "count_validated_single_param_files = 0\n",
    "count_validated_multi_param_files = 0\n",
    "count_find_by_scemantic = 0\n",
    "count_find_by_scemantic_single = 0\n",
    "count_find_by_scemantic_multi = 0\n",
    "scemantic_case = []\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    files = [x for x in files if x.endswith(\"_example_test.json\")]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "        # if json file is larger than 10MB, skip it\n",
    "        if os.path.getsize(file_path) > 100*1024*1024:\n",
    "            continue\n",
    "        endpoint_result = json.load(open(file_path))\n",
    "        endpoint_result['validated_parameters'] = {}\n",
    "        print(f\"Validating {folder}.{endpoint_result['endpoint']}\")\n",
    "        saved_error_responses = set() # API responses usually have similar error messages, save them to avoid repeated evaluation\n",
    "        if endpoint_result['tests']:\n",
    "            if type(endpoint_result['tests'][0]['gt_param']) == str:\n",
    "                count_single_param_files += 1\n",
    "            else:\n",
    "                count_multi_param_files += 1\n",
    "        for test in endpoint_result['tests']:\n",
    "            status_code = test['status_code']\n",
    "            if status_code == 200:\n",
    "                response_text = test['text']\n",
    "                # truncate the response text to avoid the max token limit of gpt\n",
    "                response_text = response_text[:500] if len(response_text) > 500 else response_text\n",
    "                if response_text in saved_error_responses:\n",
    "                    continue\n",
    "                else:\n",
    "                    # if type(test['gt_param']) == str:\n",
    "                    #     continue\n",
    "                    gpt_response = gpt_evaluate(endpoint_result['endpoint'],response_text)\n",
    "                    if gpt_response == 'error':\n",
    "                        print(f\"Error response detected: {response_text}\")\n",
    "                        saved_error_responses.add(response_text)\n",
    "                    else:\n",
    "                        print(f\"Information response detected: {response_text}\")\n",
    "                        count_validated_files += 1\n",
    "                        if type(test['gt_param']) == str:\n",
    "                            endpoint_result['validated_parameters'][test['gt_param']] = test['candidate']\n",
    "                            count_validated_single_param_files += 1\n",
    "                            if not test['test_param'] in endpoint_result['extracted_parameters']:\n",
    "                                count_find_by_scemantic += 1\n",
    "                                count_find_by_scemantic_single += 1\n",
    "                                scemantic_case.append((folder, endpoint_result['endpoint'], test['test_param'], test['gt_param']))\n",
    "                        else:\n",
    "                            count_validated_multi_param_files += 1\n",
    "                            for i, gt_param in enumerate(test['gt_param']):\n",
    "                                endpoint_result['validated_parameters'][gt_param] = test['candidate'][i]\n",
    "                                if not test['test_param'][i] in endpoint_result['extracted_parameters']:\n",
    "                                    count_find_by_scemantic += 1\n",
    "                                    count_find_by_scemantic_multi += 1\n",
    "                                    scemantic_case.append((folder, endpoint_result['endpoint'], test['test_param'][i], gt_param))\n",
    "                        break\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(endpoint_result, f)\n",
    "print(f\"Validated {count_validated_files} parameters in {count_multi_param_files+count_single_param_files} files. {count_find_by_scemantic} parameters are found by semantic search.\") \n",
    "print(f\"Validated {count_validated_single_param_files} single parameters in {count_single_param_files} files. {count_find_by_scemantic_single} single parameters are found by semantic search.\")\n",
    "print(f\"Validated {count_validated_multi_param_files} multi parameters in {count_multi_param_files} files. {count_find_by_scemantic_multi} multi parameters are found by semantic search.\")\n",
    "print(scemantic_case)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import shutil\n",
    "from typing import Dict, List, Any, Set, Optional, Union\n",
    "\n",
    "# Cell 39: Process Validated Parameters\n",
    "def process_validated_parameters(apidocs_dir: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process test results to extract validated parameters.\n",
    "    \n",
    "    Args:\n",
    "        apidocs_dir: Directory containing API documentation and test results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping API endpoints to their validated parameters\n",
    "    \"\"\"\n",
    "    validated_params = {}\n",
    "    folders = os.listdir(apidocs_dir)\n",
    "    \n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(apidocs_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(folder_path)\n",
    "        test_files = [x for x in files if x.endswith(\"_example_test.json\")]\n",
    "        \n",
    "        for file_name in test_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    test_data = json.load(f)\n",
    "                    if 'validated_parameters' in test_data and test_data['validated_parameters']:\n",
    "                        validated_params[f\"{folder}.{test_data['endpoint']}\"] = test_data['validated_parameters']\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return validated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Infer Parameter Types\n",
    "def infer_parameter_type(value: Any) -> str:\n",
    "    \"\"\"\n",
    "    Infer the type of a parameter value.\n",
    "    \n",
    "    Args:\n",
    "        value: The parameter value\n",
    "        \n",
    "    Returns:\n",
    "        A string representing the inferred type\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return \"None\"\n",
    "    elif isinstance(value, bool):\n",
    "        return \"bool\"\n",
    "    elif isinstance(value, int):\n",
    "        return \"int\"\n",
    "    elif isinstance(value, float):\n",
    "        return \"float\"\n",
    "    elif isinstance(value, str):\n",
    "        # Check if it's a date string\n",
    "        if len(value) == 10 and value.count('-') == 2:\n",
    "            try:\n",
    "                datetime.datetime.strptime(value, '%Y-%m-%d')\n",
    "                return \"date\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "        # Check if it's a JSON string\n",
    "        if (value.startswith('{') and value.endswith('}')) or (value.startswith('[') and value.endswith(']')):\n",
    "            try:\n",
    "                json.loads(value)\n",
    "                return \"json\"\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return \"str\"\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return \"list\"\n",
    "    elif isinstance(value, dict):\n",
    "        return \"dict\"\n",
    "    else:\n",
    "        return type(value).__name__\n",
    "\n",
    "def analyze_parameter_types(validated_params: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze and infer types for validated parameters.\n",
    "    \n",
    "    Args:\n",
    "        validated_params: Dictionary of validated parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with parameter types added\n",
    "    \"\"\"\n",
    "    typed_params = {}\n",
    "    \n",
    "    for tool_id, params in validated_params.items():\n",
    "        typed_params[tool_id] = {}\n",
    "        for param_name, param_value in params.items():\n",
    "            param_type = infer_parameter_type(param_value)\n",
    "            typed_params[tool_id][param_name] = {\n",
    "                'value': param_value,\n",
    "                'type': param_type\n",
    "            }\n",
    "    \n",
    "    return typed_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 41: Update Tool Code\n",
    "def update_tool_code(api_name: str, endpoint_name: str, params: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Update tool code with validated parameters.\n",
    "    \n",
    "    Args:\n",
    "        api_name: Name of the API\n",
    "        endpoint_name: Name of the endpoint\n",
    "        params: Dictionary of parameter names and their info (value and type)\n",
    "        \n",
    "    Returns:\n",
    "        True if update was successful, False otherwise\n",
    "    \"\"\"\n",
    "    tool_path = os.path.join(\"../extractor/apidocs\", api_name, f\"{endpoint_name}.py\")\n",
    "    \n",
    "    if not os.path.exists(tool_path):\n",
    "        print(f\"Tool file not found: {tool_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create a backup of the original file\n",
    "        backup_path = f\"{tool_path}.bak\"\n",
    "        shutil.copy2(tool_path, backup_path)\n",
    "        \n",
    "        # Read the original code\n",
    "        with open(tool_path, 'r') as f:\n",
    "            code = f.read()\n",
    "        \n",
    "        # Parse the code\n",
    "        tree = ast.parse(code)\n",
    "        function_name = endpoint_name.replace(\"_GET\", \"\").replace(\"_POST\", \"\")\n",
    "        \n",
    "        # Find the main function in the file\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef) and node.name == function_name:\n",
    "                # Get existing docstring\n",
    "                docstring = ast.get_docstring(node)\n",
    "                \n",
    "                # Update docstring with validated parameters\n",
    "                param_docs = \"\\n\\nValidated Parameters:\\n\"\n",
    "                for param_name, param_info in params.items():\n",
    "                    param_value = param_info['value']\n",
    "                    param_type = param_info['type']\n",
    "                    param_docs += f\"    {param_name} ({param_type}): {param_value}\\n\"\n",
    "                \n",
    "                if docstring:\n",
    "                    # Remove existing \"Validated Parameters\" section if it exists\n",
    "                    if \"Validated Parameters:\" in docstring:\n",
    "                        parts = docstring.split(\"Validated Parameters:\")\n",
    "                        docstring = parts[0].rstrip()\n",
    "                    \n",
    "                    # Add new section\n",
    "                    new_docstring = f\"{docstring}{param_docs}\"\n",
    "                    \n",
    "                    # Replace docstring in function\n",
    "                    node.body[0] = ast.Expr(value=ast.Constant(value=new_docstring))\n",
    "                else:\n",
    "                    # Create a new docstring\n",
    "                    new_docstring = f\"Function for {endpoint_name}.{param_docs}\"\n",
    "                    node.body.insert(0, ast.Expr(value=ast.Constant(value=new_docstring)))\n",
    "                \n",
    "                # Update default parameter values\n",
    "                param_values = {name: info['value'] for name, info in params.items()}\n",
    "                for i, arg in enumerate(node.args.args):\n",
    "                    if arg.arg in param_values:\n",
    "                        # Check if we need to add or update a default\n",
    "                        if i >= len(node.args.defaults):\n",
    "                            # Add a new default\n",
    "                            node.args.defaults.append(ast.Constant(value=param_values[arg.arg]))\n",
    "                        else:\n",
    "                            # Update existing default\n",
    "                            node.args.defaults[i] = ast.Constant(value=param_values[arg.arg])\n",
    "        \n",
    "        # Write updated code\n",
    "        updated_code = ast.unparse(tree)\n",
    "        with open(tool_path, 'w') as f:\n",
    "            f.write(updated_code)\n",
    "        \n",
    "        print(f\"Successfully updated {tool_path}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating tool code for {api_name}.{endpoint_name}: {e}\")\n",
    "        # Restore from backup\n",
    "        if os.path.exists(backup_path):\n",
    "            shutil.copy2(backup_path, tool_path)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 42: Create Parameter Knowledge Base\n",
    "def create_parameter_kb(validated_params: Dict, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a knowledge base from validated parameters.\n",
    "    \n",
    "    Args:\n",
    "        validated_params: Dictionary of validated parameters\n",
    "        output_path: Path to save the knowledge base\n",
    "    \"\"\"\n",
    "    kb = {\n",
    "        'tools': {},\n",
    "        'parameters': {},\n",
    "        'metadata': {\n",
    "            'created_at': datetime.datetime.now().isoformat(),\n",
    "            'total_endpoints': len(validated_params),\n",
    "            'total_parameters': sum(len(params) for params in validated_params.values())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Organize by tool\n",
    "    for tool_id, params in validated_params.items():\n",
    "        api_name, endpoint_name = tool_id.split('.')\n",
    "        \n",
    "        # Add to tools section\n",
    "        if api_name not in kb['tools']:\n",
    "            kb['tools'][api_name] = {}\n",
    "        \n",
    "        kb['tools'][api_name][endpoint_name] = params\n",
    "        \n",
    "        # Also organize by parameter for cross-tool lookup\n",
    "        for param_name, param_info in params.items():\n",
    "            if param_name not in kb['parameters']:\n",
    "                kb['parameters'][param_name] = []\n",
    "            \n",
    "            kb['parameters'][param_name].append({\n",
    "                'tool_id': tool_id,\n",
    "                'value': param_info['value'],\n",
    "                'type': param_info['type']\n",
    "            })\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(kb, f, indent=2)\n",
    "    \n",
    "    print(f\"Knowledge base created at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 43: Test the Knowledge Base\n",
    "def test_knowledge_base(kb_path: str, api_name: str = None, endpoint_name: str = None, param_name: str = None):\n",
    "    \"\"\"\n",
    "    Test the knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        kb_path: Path to the knowledge base\n",
    "        api_name: Optional API name to test\n",
    "        endpoint_name: Optional endpoint name to test\n",
    "        param_name: Optional parameter name to test\n",
    "    \"\"\"\n",
    "    # Check if the knowledge base exists\n",
    "    if not os.path.exists(kb_path):\n",
    "        print(f\"Knowledge base not found at {kb_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the knowledge base\n",
    "    with open(kb_path, 'r') as f:\n",
    "        kb = json.load(f)\n",
    "    \n",
    "    print(f\"Knowledge Base Metadata:\")\n",
    "    print(f\"  Created: {kb['metadata']['created_at']}\")\n",
    "    print(f\"  Total Endpoints: {kb['metadata']['total_endpoints']}\")\n",
    "    print(f\"  Total Parameters: {kb['metadata']['total_parameters']}\")\n",
    "    \n",
    "    # Print APIs and endpoints\n",
    "    print(\"\\nAPIs and Endpoints:\")\n",
    "    for api, endpoints in kb['tools'].items():\n",
    "        print(f\"  {api}:\")\n",
    "        for endpoint in endpoints:\n",
    "            print(f\"    - {endpoint}\")\n",
    "    \n",
    "    # Print parameters\n",
    "    print(\"\\nParameters:\")\n",
    "    param_count = len(kb['parameters'])\n",
    "    print(f\"  Total unique parameters: {param_count}\")\n",
    "    if param_count > 0:\n",
    "        print(\"  Sample parameters:\")\n",
    "        for i, (param, examples) in enumerate(kb['parameters'].items()):\n",
    "            if i >= 5:  # Show only 5 examples\n",
    "                break\n",
    "            print(f\"    - {param}: {len(examples)} examples\")\n",
    "    \n",
    "    # Test specific API/endpoint if provided\n",
    "    if api_name and api_name in kb['tools']:\n",
    "        print(f\"\\nTesting API: {api_name}\")\n",
    "        if endpoint_name and endpoint_name in kb['tools'][api_name]:\n",
    "            print(f\"  Endpoint: {endpoint_name}\")\n",
    "            params = kb['tools'][api_name][endpoint_name]\n",
    "            print(f\"  Parameters:\")\n",
    "            for param_name, param_info in params.items():\n",
    "                print(f\"    - {param_name}: {param_info['value']} ({param_info['type']})\")\n",
    "    \n",
    "    # Test specific parameter if provided\n",
    "    if param_name and param_name in kb['parameters']:\n",
    "        print(f\"\\nTesting Parameter: {param_name}\")\n",
    "        examples = kb['parameters'][param_name]\n",
    "        print(f\"  {len(examples)} examples found:\")\n",
    "        for example in examples:\n",
    "            print(f\"    - {example['value']} ({example['type']}) from {example['tool_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 44: Run Refinement Process\n",
    "def run_refinement_process():\n",
    "    \"\"\"Run the complete tool refinement process.\"\"\"\n",
    "    apidocs_dir = \"../extractor/apidocs\"\n",
    "    kb_path = \"../utils/parameter_kb.json\"\n",
    "    \n",
    "    print(\"🏭 Starting ToolFactory Refinement Process 🏭\")\n",
    "    print(\"\\n1️⃣ Processing validated parameters...\")\n",
    "    validated_params = process_validated_parameters(apidocs_dir)\n",
    "    print(f\"   Found {len(validated_params)} tools with validated parameters\")\n",
    "    \n",
    "    print(\"\\n2️⃣ Analyzing parameter types...\")\n",
    "    typed_params = analyze_parameter_types(validated_params)\n",
    "    print(f\"   Analyzed types for {sum(len(params) for params in typed_params.values())} parameters\")\n",
    "    \n",
    "    print(\"\\n3️⃣ Updating tool code with validated parameters...\")\n",
    "    updated_count = 0\n",
    "    for tool_id, params in typed_params.items():\n",
    "        api_name, endpoint_name = tool_id.split('.')\n",
    "        if update_tool_code(api_name, endpoint_name, params):\n",
    "            updated_count += 1\n",
    "    print(f\"   Updated {updated_count} out of {len(typed_params)} tools\")\n",
    "    \n",
    "    print(\"\\n4️⃣ Creating parameter knowledge base...\")\n",
    "    create_parameter_kb(typed_params, kb_path)\n",
    "    \n",
    "    print(\"\\n✅ Tool refinement process completed!\")\n",
    "    print(f\"   - Knowledge base available at: {kb_path}\")\n",
    "    \n",
    "    return typed_params, kb_path\n",
    "\n",
    "# Uncomment to run the process\n",
    "# typed_params, kb_path = run_refinement_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 45: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the refinement process\n",
    "    typed_params, kb_path = run_refinement_process()\n",
    "    \n",
    "    # Test the knowledge base\n",
    "    print(\"\\n🧪 Testing the Knowledge Base:\")\n",
    "    test_knowledge_base(kb_path)\n",
    "    \n",
    "    print(\"\\n📝 Next Steps:\")\n",
    "    print(\"1. The knowledge base has been created at:\", kb_path)\n",
    "    print(\"2. Tool implementations have been updated with validated parameters\")\n",
    "    print(\"3. Use the tool_kb_interface.py module in your agents to access the knowledge base\")\n",
    "    print(\"   - Import the ToolKnowledgeBase class from tool_kb_interface\")\n",
    "    print(\"   - Create an instance with kb = ToolKnowledgeBase()\")\n",
    "    print(\"   - Use kb.suggest_parameter_value(api_name, endpoint_name, param_name) to get suggested values\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "API Agent",
   "language": "python",
   "name": "apiagent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
